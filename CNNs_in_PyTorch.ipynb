{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7xhBu3yK1Fht"
      },
      "outputs": [],
      "source": [
        "# Load pytorch modules\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# what I learnt:\n",
        "\n",
        "*   dataloader and torchvision transforms\n",
        "\n",
        "*   Preloaded vision transforms\n",
        "\n",
        "# What I could have done better:\n",
        "\n",
        "\n",
        "*   \n",
        "*   List item\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7CgfIyfSct4k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4mNlXI0r619h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02a2814a-88e2-47ab-be10-ee64d056ad27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 19.5MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 337kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 6.23MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 11.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "# split to training and test data\n",
        "train_data = FashionMNIST(root = 'data/', download = True, train = True, transform = ToTensor())\n",
        "test_data = FashionMNIST(root = 'data/', download = True, train = False, transform=ToTensor())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ap0EoHKrz22C"
      },
      "outputs": [],
      "source": [
        "labels_map = {\n",
        "    0: \"T-Shirt\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle Boot\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "a6YAgaf6zsTC"
      },
      "outputs": [],
      "source": [
        "#Wrap in dataloader to split batch size\n",
        "\n",
        "train_dataloader = DataLoader(train_data, batch_size = 64, shuffle = True, num_workers = 2)\n",
        "test_dataloader = DataLoader(test_data, batch_size = 64, shuffle = False,num_workers = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "DHVInacS1evW",
        "outputId": "65c86af4-305c-4a78-f9d8-3010a66f3807"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
            "Labels batch shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: Ankle Boot\n"
          ]
        }
      ],
      "source": [
        "# visualising stuff in the train features\n",
        "import matplotlib.pyplot as plt\n",
        "train_data, train_label = next(iter(train_dataloader)) # runs through the iterable dataloader\n",
        "print(f\"Feature batch shape: {train_data.size()}\")\n",
        "print(f\"Labels batch shape: {train_label.size()}\")\n",
        "img = train_data[1].squeeze()\n",
        "label = train_label[1]\n",
        "plt.imshow(img, cmap=\"gray\")\n",
        "plt.show()\n",
        "print(f\"Label: {labels_map[label.item()]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# what I learnt:\n",
        "1. next and iter\n",
        "\n",
        "#what i could do better:\n",
        "1. look to see whether I can find a modular code for any manual process i can build."
      ],
      "metadata": {
        "id": "ViBe_IJXrVtF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TYFOhOjr64kZ"
      },
      "outputs": [],
      "source": [
        "# Build model\n",
        "# 1. Define model architecture\n",
        "# 2. Work on working on input flow into the model after designing model architecture.\n",
        "# 3. work on seeing if there are helper functions\n",
        "\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=3, padding= 1)\n",
        "    self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=3, padding =1)\n",
        "    self.conv3 = nn.Conv2d(in_channels =12, out_channels = 32, kernel_size=3, padding=1)\n",
        "    self.conv4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "    self.fc1 = nn.LazyLinear(out_features= 1024)\n",
        "    self.fc2 = nn.LazyLinear(out_features= 10)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "    x = F.relu(self.conv3(x))\n",
        "    x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "    x = F.relu(self.conv4(x))\n",
        "    x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.fc1(x)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# what I could have done better:\n",
        "first pull out the formula for calculating the shape of the pixel arrays.\n",
        "when designing the architecture.  \n",
        "pick up a pen and paper and design the architecture and it's part or if using a notion note, just design in there.\n",
        "\n",
        "# what i learnt:\n",
        "LazyLinear to avoid calculating input shape"
      ],
      "metadata": {
        "id": "A0aIyXubBpJH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sNrzmP74OF3d"
      },
      "outputs": [],
      "source": [
        "model = CNN()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Lg8S_AjTCzXm"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "optimizer = Adam(model.parameters(),lr = 0.01)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgAjFGK5DP71",
        "outputId": "ea36d2b9-f059-4a1b-dda9-aaaed5b2652e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150 — Loss: 0.6033, Acc: 0.7758\n",
            "Epoch 2/150 — Loss: 0.4578, Acc: 0.8368\n",
            "Epoch 3/150 — Loss: 0.4251, Acc: 0.8481\n",
            "Epoch 4/150 — Loss: 0.4057, Acc: 0.8548\n",
            "Epoch 5/150 — Loss: 0.3952, Acc: 0.8586\n",
            "Epoch 6/150 — Loss: 0.3884, Acc: 0.8624\n",
            "Epoch 7/150 — Loss: 0.3890, Acc: 0.8624\n",
            "Epoch 8/150 — Loss: 0.3816, Acc: 0.8640\n",
            "Epoch 9/150 — Loss: 0.3761, Acc: 0.8665\n",
            "Epoch 10/150 — Loss: 0.3802, Acc: 0.8654\n",
            "Epoch 11/150 — Loss: 0.3731, Acc: 0.8682\n",
            "Epoch 12/150 — Loss: 0.3736, Acc: 0.8671\n",
            "Epoch 13/150 — Loss: 0.3790, Acc: 0.8664\n",
            "Epoch 14/150 — Loss: 0.3633, Acc: 0.8710\n",
            "Epoch 15/150 — Loss: 0.3802, Acc: 0.8663\n",
            "Epoch 16/150 — Loss: 0.3720, Acc: 0.8688\n",
            "Epoch 17/150 — Loss: 0.3617, Acc: 0.8714\n",
            "Epoch 18/150 — Loss: 0.3684, Acc: 0.8694\n",
            "Epoch 19/150 — Loss: 0.3667, Acc: 0.8703\n",
            "Epoch 20/150 — Loss: 0.3651, Acc: 0.8704\n",
            "Epoch 21/150 — Loss: 0.3574, Acc: 0.8731\n",
            "Epoch 22/150 — Loss: 0.3616, Acc: 0.8707\n",
            "Epoch 23/150 — Loss: 0.3561, Acc: 0.8736\n",
            "Epoch 24/150 — Loss: 0.3678, Acc: 0.8705\n",
            "Epoch 25/150 — Loss: 0.3666, Acc: 0.8705\n",
            "Epoch 26/150 — Loss: 0.3658, Acc: 0.8715\n",
            "Epoch 27/150 — Loss: 0.3569, Acc: 0.8724\n",
            "Epoch 28/150 — Loss: 0.3666, Acc: 0.8695\n",
            "Epoch 29/150 — Loss: 0.5167, Acc: 0.8266\n",
            "Epoch 30/150 — Loss: 0.3494, Acc: 0.8739\n",
            "Epoch 31/150 — Loss: 0.3437, Acc: 0.8763\n",
            "Epoch 32/150 — Loss: 0.3540, Acc: 0.8747\n",
            "Epoch 33/150 — Loss: 0.3915, Acc: 0.8665\n",
            "Epoch 34/150 — Loss: 0.3777, Acc: 0.8680\n",
            "Epoch 35/150 — Loss: 0.3675, Acc: 0.8695\n",
            "Epoch 36/150 — Loss: 0.3525, Acc: 0.8749\n",
            "Epoch 37/150 — Loss: 0.3658, Acc: 0.8707\n",
            "Epoch 38/150 — Loss: 0.3514, Acc: 0.8756\n",
            "Epoch 39/150 — Loss: 0.3597, Acc: 0.8733\n",
            "Epoch 40/150 — Loss: 0.3625, Acc: 0.8721\n",
            "Epoch 41/150 — Loss: 0.3504, Acc: 0.8748\n",
            "Epoch 42/150 — Loss: 0.4489, Acc: 0.8454\n",
            "Epoch 43/150 — Loss: 0.3696, Acc: 0.8694\n",
            "Epoch 44/150 — Loss: 0.3575, Acc: 0.8716\n",
            "Epoch 45/150 — Loss: 0.3654, Acc: 0.8706\n",
            "Epoch 46/150 — Loss: 0.3646, Acc: 0.8695\n",
            "Epoch 47/150 — Loss: 0.3609, Acc: 0.8712\n",
            "Epoch 48/150 — Loss: 0.3707, Acc: 0.8677\n",
            "Epoch 49/150 — Loss: 0.3695, Acc: 0.8695\n",
            "Epoch 50/150 — Loss: 0.3559, Acc: 0.8723\n",
            "Epoch 51/150 — Loss: 0.3666, Acc: 0.8691\n",
            "Epoch 52/150 — Loss: 0.3634, Acc: 0.8725\n",
            "Epoch 53/150 — Loss: 0.3671, Acc: 0.8714\n",
            "Epoch 54/150 — Loss: 0.3573, Acc: 0.8732\n",
            "Epoch 55/150 — Loss: 0.3765, Acc: 0.8678\n",
            "Epoch 56/150 — Loss: 0.3714, Acc: 0.8713\n",
            "Epoch 57/150 — Loss: 0.3571, Acc: 0.8720\n",
            "Epoch 58/150 — Loss: 0.3620, Acc: 0.8720\n",
            "Epoch 59/150 — Loss: 0.3689, Acc: 0.8700\n",
            "Epoch 60/150 — Loss: 0.3713, Acc: 0.8701\n",
            "Epoch 61/150 — Loss: 0.3581, Acc: 0.8725\n",
            "Epoch 62/150 — Loss: 0.3723, Acc: 0.8697\n",
            "Epoch 63/150 — Loss: 0.3555, Acc: 0.8720\n",
            "Epoch 64/150 — Loss: 0.3552, Acc: 0.8734\n",
            "Epoch 65/150 — Loss: 0.3637, Acc: 0.8717\n",
            "Epoch 66/150 — Loss: 0.3921, Acc: 0.8627\n",
            "Epoch 67/150 — Loss: 0.3488, Acc: 0.8742\n",
            "Epoch 68/150 — Loss: 0.3523, Acc: 0.8743\n",
            "Epoch 69/150 — Loss: 0.3764, Acc: 0.8666\n",
            "Epoch 70/150 — Loss: 0.3601, Acc: 0.8714\n",
            "Epoch 71/150 — Loss: 0.3617, Acc: 0.8725\n",
            "Epoch 72/150 — Loss: 0.3674, Acc: 0.8693\n",
            "Epoch 73/150 — Loss: 0.3621, Acc: 0.8714\n",
            "Epoch 74/150 — Loss: 0.3779, Acc: 0.8676\n",
            "Epoch 75/150 — Loss: 0.4381, Acc: 0.8518\n",
            "Epoch 76/150 — Loss: 0.3643, Acc: 0.8700\n",
            "Epoch 77/150 — Loss: 0.3507, Acc: 0.8747\n",
            "Epoch 78/150 — Loss: 0.3451, Acc: 0.8761\n",
            "Epoch 79/150 — Loss: 0.3652, Acc: 0.8707\n",
            "Epoch 80/150 — Loss: 0.4563, Acc: 0.8439\n",
            "Epoch 81/150 — Loss: 0.3717, Acc: 0.8654\n",
            "Epoch 82/150 — Loss: 0.3463, Acc: 0.8746\n",
            "Epoch 83/150 — Loss: 0.3522, Acc: 0.8743\n",
            "Epoch 84/150 — Loss: 0.3592, Acc: 0.8732\n",
            "Epoch 85/150 — Loss: 0.3805, Acc: 0.8659\n",
            "Epoch 86/150 — Loss: 0.3594, Acc: 0.8728\n",
            "Epoch 87/150 — Loss: 0.3613, Acc: 0.8734\n",
            "Epoch 88/150 — Loss: 0.4325, Acc: 0.8528\n",
            "Epoch 89/150 — Loss: 0.3566, Acc: 0.8729\n",
            "Epoch 90/150 — Loss: 0.3507, Acc: 0.8743\n",
            "Epoch 91/150 — Loss: 0.3593, Acc: 0.8723\n",
            "Epoch 92/150 — Loss: 0.4278, Acc: 0.8549\n",
            "Epoch 93/150 — Loss: 0.3541, Acc: 0.8733\n",
            "Epoch 94/150 — Loss: 0.3535, Acc: 0.8747\n",
            "Epoch 95/150 — Loss: 0.3598, Acc: 0.8723\n",
            "Epoch 96/150 — Loss: 0.3492, Acc: 0.8747\n",
            "Epoch 97/150 — Loss: 0.3686, Acc: 0.8691\n",
            "Epoch 98/150 — Loss: 0.3720, Acc: 0.8701\n",
            "Epoch 99/150 — Loss: 0.3788, Acc: 0.8687\n",
            "Epoch 100/150 — Loss: 0.3536, Acc: 0.8727\n",
            "Epoch 101/150 — Loss: 0.3549, Acc: 0.8742\n",
            "Epoch 102/150 — Loss: 0.4242, Acc: 0.8580\n",
            "Epoch 103/150 — Loss: 0.3726, Acc: 0.8674\n",
            "Epoch 104/150 — Loss: 0.3581, Acc: 0.8711\n",
            "Epoch 105/150 — Loss: 0.3600, Acc: 0.8711\n",
            "Epoch 106/150 — Loss: 0.5828, Acc: 0.8131\n",
            "Epoch 107/150 — Loss: 0.4061, Acc: 0.8597\n",
            "Epoch 108/150 — Loss: 0.3657, Acc: 0.8705\n",
            "Epoch 109/150 — Loss: 0.3601, Acc: 0.8708\n",
            "Epoch 110/150 — Loss: 0.3532, Acc: 0.8731\n",
            "Epoch 111/150 — Loss: 0.3620, Acc: 0.8719\n",
            "Epoch 112/150 — Loss: 0.3785, Acc: 0.8674\n",
            "Epoch 113/150 — Loss: 0.3615, Acc: 0.8718\n",
            "Epoch 114/150 — Loss: 0.3688, Acc: 0.8685\n",
            "Epoch 115/150 — Loss: 0.3635, Acc: 0.8698\n",
            "Epoch 116/150 — Loss: 0.3532, Acc: 0.8748\n",
            "Epoch 117/150 — Loss: 0.5225, Acc: 0.8266\n",
            "Epoch 118/150 — Loss: 0.4388, Acc: 0.8471\n",
            "Epoch 119/150 — Loss: 0.3738, Acc: 0.8664\n",
            "Epoch 120/150 — Loss: 0.3444, Acc: 0.8757\n",
            "Epoch 121/150 — Loss: 0.3594, Acc: 0.8710\n",
            "Epoch 122/150 — Loss: 0.4472, Acc: 0.8522\n",
            "Epoch 123/150 — Loss: 0.3869, Acc: 0.8627\n",
            "Epoch 124/150 — Loss: 0.3444, Acc: 0.8751\n",
            "Epoch 125/150 — Loss: 0.3487, Acc: 0.8752\n",
            "Epoch 126/150 — Loss: 0.3491, Acc: 0.8764\n",
            "Epoch 127/150 — Loss: 0.4376, Acc: 0.8508\n",
            "Epoch 128/150 — Loss: 0.3447, Acc: 0.8749\n",
            "Epoch 129/150 — Loss: 0.3527, Acc: 0.8743\n",
            "Epoch 130/150 — Loss: 0.4101, Acc: 0.8603\n",
            "Epoch 131/150 — Loss: 0.3542, Acc: 0.8732\n",
            "Epoch 132/150 — Loss: 0.3441, Acc: 0.8751\n",
            "Epoch 133/150 — Loss: 0.4313, Acc: 0.8546\n",
            "Epoch 134/150 — Loss: 0.3648, Acc: 0.8702\n",
            "Epoch 135/150 — Loss: 0.3640, Acc: 0.8712\n",
            "Epoch 136/150 — Loss: 0.3575, Acc: 0.8728\n",
            "Epoch 137/150 — Loss: 0.3593, Acc: 0.8722\n",
            "Epoch 138/150 — Loss: 0.3731, Acc: 0.8686\n",
            "Epoch 139/150 — Loss: 0.3984, Acc: 0.8613\n",
            "Epoch 140/150 — Loss: 0.3801, Acc: 0.8673\n",
            "Epoch 141/150 — Loss: 0.4493, Acc: 0.8506\n",
            "Epoch 142/150 — Loss: 0.3705, Acc: 0.8708\n",
            "Epoch 143/150 — Loss: 0.3445, Acc: 0.8767\n",
            "Epoch 144/150 — Loss: 0.3588, Acc: 0.8717\n",
            "Epoch 145/150 — Loss: 0.3623, Acc: 0.8709\n",
            "Epoch 146/150 — Loss: 0.3671, Acc: 0.8708\n",
            "Epoch 147/150 — Loss: 0.4846, Acc: 0.8391\n",
            "Epoch 148/150 — Loss: 0.4085, Acc: 0.8566\n",
            "Epoch 149/150 — Loss: 0.3965, Acc: 0.8617\n",
            "Epoch 150/150 — Loss: 0.3967, Acc: 0.8627\n"
          ]
        }
      ],
      "source": [
        "epochs = 150\n",
        "train_loss = []\n",
        "train_acc = []\n",
        "test_loss = []\n",
        "test_acc = []\n",
        "model.train()  # set training mode\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0\n",
        "    epoch_correct = 0\n",
        "    epoch_total = 0\n",
        "\n",
        "    for x, y in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(x)                      # call model safely\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # accumulate metrics\n",
        "        epoch_loss += loss.item() * x.size(0)\n",
        "        epoch_correct += (y_pred.argmax(1) == y).sum().item()\n",
        "        epoch_total += x.size(0)\n",
        "\n",
        "    train_loss.append(epoch_loss / epoch_total)\n",
        "    train_acc.append(epoch_correct / epoch_total)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} — Loss: {train_loss[-1]:.4f}, Acc: {train_acc[-1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#what i learnt:\n",
        "1. calling model.foward(x) directly just pulls out the forward method and doesn't taking into account other stuff built into the model like train and evaluate.\n",
        "2. Don't assume the way you learnt how to build stuff is the standard way.\n",
        "\n",
        "# what i could do better:\n",
        "1. Look for the standard way a framework is used and build adaptations from there.\n",
        "2. Don't try to adapt with limited knowledge. Recipe for error."
      ],
      "metadata": {
        "id": "7CAIGp8_B_87"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0FKMQySDRYP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}