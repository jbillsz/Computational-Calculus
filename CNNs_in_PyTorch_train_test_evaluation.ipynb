{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7xhBu3yK1Fht"
      },
      "outputs": [],
      "source": [
        "# Load pytorch modules\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# what I learnt:\n",
        "\n",
        "*   dataloader and torchvision transforms\n",
        "\n",
        "*   Preloaded vision transforms\n",
        "\n",
        "# What I could have done better:\n",
        "\n",
        "\n",
        "*   \n",
        "*   List item\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7CgfIyfSct4k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4mNlXI0r619h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0a0d118-3d77-41b8-e4a5-c7e6a589b361"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 21.3MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 346kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 6.29MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 17.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "# split to training and test data\n",
        "train_data = FashionMNIST(root = 'data/', download = True, train = True, transform = ToTensor())\n",
        "test_data = FashionMNIST(root = 'data/', download = True, train = False, transform=ToTensor())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ap0EoHKrz22C"
      },
      "outputs": [],
      "source": [
        "labels_map = {\n",
        "    0: \"T-Shirt\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle Boot\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "a6YAgaf6zsTC"
      },
      "outputs": [],
      "source": [
        "#Wrap in dataloader to split batch size\n",
        "\n",
        "train_dataloader = DataLoader(train_data, batch_size = 64, shuffle = True, num_workers = 2)\n",
        "test_dataloader = DataLoader(test_data, batch_size = 64, shuffle = False,num_workers = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "DHVInacS1evW",
        "outputId": "39045b62-0abe-4298-8a0f-4d50f0096360"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
            "Labels batch shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: Sneaker\n"
          ]
        }
      ],
      "source": [
        "# visualising stuff in the train features\n",
        "import matplotlib.pyplot as plt\n",
        "train_data, train_label = next(iter(train_dataloader)) # runs through the iterable dataloader\n",
        "print(f\"Feature batch shape: {train_data.size()}\")\n",
        "print(f\"Labels batch shape: {train_label.size()}\")\n",
        "img = train_data[1].squeeze()\n",
        "label = train_label[1]\n",
        "plt.imshow(img, cmap=\"gray\")\n",
        "plt.show()\n",
        "print(f\"Label: {labels_map[label.item()]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# what I learnt:\n",
        "1. next and iter\n",
        "\n",
        "#what i could do better:\n",
        "1. look to see whether I can find a modular code for any manual process i can build."
      ],
      "metadata": {
        "id": "ViBe_IJXrVtF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TYFOhOjr64kZ"
      },
      "outputs": [],
      "source": [
        "# Build model\n",
        "# 1. Define model architecture\n",
        "# 2. Work on working on input flow into the model after designing model architecture.\n",
        "# 3. work on seeing if there are helper functions\n",
        "\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=3, padding= 1)\n",
        "    self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=3, padding =1)\n",
        "    self.conv3 = nn.Conv2d(in_channels =12, out_channels = 32, kernel_size=3, padding=1)\n",
        "    self.conv4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "    self.fc1 = nn.LazyLinear(out_features= 1024)\n",
        "    self.fc2 = nn.LazyLinear(out_features= 10)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "    x = F.relu(self.conv3(x))\n",
        "    x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "    x = F.relu(self.conv4(x))\n",
        "    x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.fc1(x)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# what I could have done better:\n",
        "first pull out the formula for calculating the shape of the pixel arrays.\n",
        "when designing the architecture.  \n",
        "pick up a pen and paper and design the architecture and it's part or if using a notion note, just design in there.\n",
        "\n",
        "# what i learnt:\n",
        "LazyLinear to avoid calculating input shape"
      ],
      "metadata": {
        "id": "A0aIyXubBpJH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sNrzmP74OF3d"
      },
      "outputs": [],
      "source": [
        "model = CNN()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Lg8S_AjTCzXm"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "optimizer = Adam(model.parameters(),lr = 0.01)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgAjFGK5DP71",
        "outputId": "33d3abef-22bf-463d-88ea-4c164f17cf99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150 — Loss: 0.6600, Acc: 0.7552\n",
            "Epoch 2/150 — Loss: 0.5068, Acc: 0.8167\n",
            "Epoch 3/150 — Loss: 0.4777, Acc: 0.8252\n",
            "Epoch 4/150 — Loss: 0.4659, Acc: 0.8292\n",
            "Epoch 5/150 — Loss: 0.4596, Acc: 0.8305\n",
            "Epoch 6/150 — Loss: 0.4519, Acc: 0.8355\n",
            "Epoch 7/150 — Loss: 0.4515, Acc: 0.8358\n",
            "Epoch 8/150 — Loss: 0.4503, Acc: 0.8360\n",
            "Epoch 9/150 — Loss: 0.4457, Acc: 0.8384\n",
            "Epoch 10/150 — Loss: 0.4399, Acc: 0.8387\n",
            "Epoch 11/150 — Loss: 0.4352, Acc: 0.8403\n",
            "Epoch 12/150 — Loss: 0.4374, Acc: 0.8404\n",
            "Epoch 13/150 — Loss: 0.4315, Acc: 0.8423\n",
            "Epoch 14/150 — Loss: 0.4356, Acc: 0.8409\n",
            "Epoch 15/150 — Loss: 0.4353, Acc: 0.8435\n",
            "Epoch 16/150 — Loss: 0.4304, Acc: 0.8422\n",
            "Epoch 17/150 — Loss: 0.4294, Acc: 0.8438\n",
            "Epoch 18/150 — Loss: 0.4236, Acc: 0.8451\n",
            "Epoch 19/150 — Loss: 0.4284, Acc: 0.8454\n",
            "Epoch 20/150 — Loss: 0.4196, Acc: 0.8470\n",
            "Epoch 21/150 — Loss: 0.4259, Acc: 0.8454\n",
            "Epoch 22/150 — Loss: 0.4296, Acc: 0.8453\n",
            "Epoch 23/150 — Loss: 0.4244, Acc: 0.8456\n",
            "Epoch 24/150 — Loss: 0.4237, Acc: 0.8463\n",
            "Epoch 25/150 — Loss: 0.4176, Acc: 0.8481\n",
            "Epoch 26/150 — Loss: 0.4183, Acc: 0.8482\n",
            "Epoch 27/150 — Loss: 0.4140, Acc: 0.8501\n",
            "Epoch 28/150 — Loss: 0.4218, Acc: 0.8492\n",
            "Epoch 29/150 — Loss: 0.4187, Acc: 0.8471\n",
            "Epoch 30/150 — Loss: 0.4125, Acc: 0.8507\n",
            "Epoch 31/150 — Loss: 0.4104, Acc: 0.8509\n",
            "Epoch 32/150 — Loss: 0.4171, Acc: 0.8497\n",
            "Epoch 33/150 — Loss: 0.4160, Acc: 0.8501\n",
            "Epoch 34/150 — Loss: 0.4154, Acc: 0.8505\n",
            "Epoch 35/150 — Loss: 0.4129, Acc: 0.8511\n",
            "Epoch 36/150 — Loss: 0.4145, Acc: 0.8507\n",
            "Epoch 37/150 — Loss: 0.4209, Acc: 0.8505\n",
            "Epoch 38/150 — Loss: 0.4160, Acc: 0.8496\n",
            "Epoch 39/150 — Loss: 0.4058, Acc: 0.8550\n",
            "Epoch 40/150 — Loss: 0.4086, Acc: 0.8530\n",
            "Epoch 41/150 — Loss: 0.4088, Acc: 0.8520\n",
            "Epoch 42/150 — Loss: 0.4190, Acc: 0.8496\n",
            "Epoch 43/150 — Loss: 0.4102, Acc: 0.8527\n",
            "Epoch 44/150 — Loss: 0.4100, Acc: 0.8548\n",
            "Epoch 45/150 — Loss: 0.4104, Acc: 0.8520\n",
            "Epoch 46/150 — Loss: 0.4037, Acc: 0.8542\n",
            "Epoch 47/150 — Loss: 0.4015, Acc: 0.8550\n",
            "Epoch 48/150 — Loss: 0.4216, Acc: 0.8512\n",
            "Epoch 49/150 — Loss: 0.4040, Acc: 0.8548\n",
            "Epoch 50/150 — Loss: 0.4124, Acc: 0.8517\n",
            "Epoch 51/150 — Loss: 0.4159, Acc: 0.8505\n",
            "Epoch 52/150 — Loss: 0.4049, Acc: 0.8535\n",
            "Epoch 53/150 — Loss: 0.4047, Acc: 0.8541\n",
            "Epoch 54/150 — Loss: 0.4113, Acc: 0.8519\n",
            "Epoch 55/150 — Loss: 0.4082, Acc: 0.8536\n",
            "Epoch 56/150 — Loss: 0.4031, Acc: 0.8542\n",
            "Epoch 57/150 — Loss: 0.4103, Acc: 0.8512\n",
            "Epoch 58/150 — Loss: 0.4044, Acc: 0.8551\n",
            "Epoch 59/150 — Loss: 0.4037, Acc: 0.8547\n",
            "Epoch 60/150 — Loss: 0.4073, Acc: 0.8521\n",
            "Epoch 61/150 — Loss: 0.4055, Acc: 0.8532\n",
            "Epoch 62/150 — Loss: 0.4015, Acc: 0.8548\n",
            "Epoch 63/150 — Loss: 0.4090, Acc: 0.8513\n",
            "Epoch 64/150 — Loss: 0.4081, Acc: 0.8532\n",
            "Epoch 65/150 — Loss: 0.4072, Acc: 0.8538\n",
            "Epoch 66/150 — Loss: 0.4045, Acc: 0.8540\n",
            "Epoch 67/150 — Loss: 0.4049, Acc: 0.8535\n",
            "Epoch 68/150 — Loss: 0.4094, Acc: 0.8519\n",
            "Epoch 69/150 — Loss: 0.4038, Acc: 0.8554\n",
            "Epoch 70/150 — Loss: 0.4178, Acc: 0.8517\n",
            "Epoch 71/150 — Loss: 0.4031, Acc: 0.8537\n",
            "Epoch 72/150 — Loss: 0.4137, Acc: 0.8525\n",
            "Epoch 73/150 — Loss: 0.4147, Acc: 0.8524\n",
            "Epoch 74/150 — Loss: 0.4034, Acc: 0.8548\n",
            "Epoch 75/150 — Loss: 0.4134, Acc: 0.8518\n",
            "Epoch 76/150 — Loss: 0.4118, Acc: 0.8529\n",
            "Epoch 77/150 — Loss: 0.4077, Acc: 0.8526\n",
            "Epoch 78/150 — Loss: 0.4100, Acc: 0.8540\n",
            "Epoch 79/150 — Loss: 0.4059, Acc: 0.8540\n",
            "Epoch 80/150 — Loss: 0.4104, Acc: 0.8530\n",
            "Epoch 81/150 — Loss: 0.4016, Acc: 0.8566\n",
            "Epoch 82/150 — Loss: 0.4050, Acc: 0.8536\n",
            "Epoch 83/150 — Loss: 0.4051, Acc: 0.8529\n",
            "Epoch 84/150 — Loss: 0.4127, Acc: 0.8519\n",
            "Epoch 85/150 — Loss: 0.4070, Acc: 0.8529\n",
            "Epoch 86/150 — Loss: 0.4149, Acc: 0.8514\n",
            "Epoch 87/150 — Loss: 0.4028, Acc: 0.8539\n",
            "Epoch 88/150 — Loss: 0.4002, Acc: 0.8546\n",
            "Epoch 89/150 — Loss: 0.4071, Acc: 0.8536\n",
            "Epoch 90/150 — Loss: 0.4045, Acc: 0.8552\n",
            "Epoch 91/150 — Loss: 0.4042, Acc: 0.8549\n",
            "Epoch 92/150 — Loss: 0.4077, Acc: 0.8545\n",
            "Epoch 93/150 — Loss: 0.4033, Acc: 0.8548\n",
            "Epoch 94/150 — Loss: 0.4053, Acc: 0.8548\n",
            "Epoch 95/150 — Loss: 0.4063, Acc: 0.8547\n",
            "Epoch 96/150 — Loss: 0.4058, Acc: 0.8547\n",
            "Epoch 97/150 — Loss: 0.4044, Acc: 0.8556\n",
            "Epoch 98/150 — Loss: 0.4043, Acc: 0.8542\n",
            "Epoch 99/150 — Loss: 0.4100, Acc: 0.8543\n",
            "Epoch 100/150 — Loss: 0.4033, Acc: 0.8558\n",
            "Epoch 101/150 — Loss: 0.4109, Acc: 0.8541\n",
            "Epoch 102/150 — Loss: 0.4101, Acc: 0.8508\n",
            "Epoch 103/150 — Loss: 0.4084, Acc: 0.8537\n",
            "Epoch 104/150 — Loss: 0.4041, Acc: 0.8548\n",
            "Epoch 105/150 — Loss: 0.4007, Acc: 0.8577\n",
            "Epoch 106/150 — Loss: 0.4065, Acc: 0.8548\n",
            "Epoch 107/150 — Loss: 0.4086, Acc: 0.8541\n",
            "Epoch 108/150 — Loss: 0.4044, Acc: 0.8542\n",
            "Epoch 109/150 — Loss: 0.4027, Acc: 0.8555\n",
            "Epoch 110/150 — Loss: 0.4104, Acc: 0.8525\n",
            "Epoch 111/150 — Loss: 0.4025, Acc: 0.8560\n",
            "Epoch 112/150 — Loss: 0.4033, Acc: 0.8537\n",
            "Epoch 113/150 — Loss: 0.4007, Acc: 0.8551\n",
            "Epoch 114/150 — Loss: 0.4149, Acc: 0.8520\n",
            "Epoch 115/150 — Loss: 0.4073, Acc: 0.8540\n",
            "Epoch 116/150 — Loss: 0.3983, Acc: 0.8545\n",
            "Epoch 117/150 — Loss: 0.4098, Acc: 0.8544\n",
            "Epoch 118/150 — Loss: 0.4079, Acc: 0.8539\n",
            "Epoch 119/150 — Loss: 0.4067, Acc: 0.8543\n",
            "Epoch 120/150 — Loss: 0.4196, Acc: 0.8512\n",
            "Epoch 121/150 — Loss: 0.4067, Acc: 0.8520\n",
            "Epoch 122/150 — Loss: 0.4034, Acc: 0.8547\n",
            "Epoch 123/150 — Loss: 0.4079, Acc: 0.8528\n",
            "Epoch 124/150 — Loss: 0.3986, Acc: 0.8560\n",
            "Epoch 125/150 — Loss: 0.4112, Acc: 0.8515\n",
            "Epoch 126/150 — Loss: 0.4200, Acc: 0.8501\n",
            "Epoch 127/150 — Loss: 0.3982, Acc: 0.8571\n",
            "Epoch 128/150 — Loss: 0.4087, Acc: 0.8535\n",
            "Epoch 129/150 — Loss: 0.4010, Acc: 0.8550\n",
            "Epoch 130/150 — Loss: 0.4082, Acc: 0.8547\n",
            "Epoch 131/150 — Loss: 0.4166, Acc: 0.8503\n",
            "Epoch 132/150 — Loss: 0.4013, Acc: 0.8550\n",
            "Epoch 133/150 — Loss: 0.4026, Acc: 0.8563\n",
            "Epoch 134/150 — Loss: 0.4180, Acc: 0.8503\n",
            "Epoch 135/150 — Loss: 0.4044, Acc: 0.8535\n",
            "Epoch 136/150 — Loss: 0.3962, Acc: 0.8584\n",
            "Epoch 137/150 — Loss: 0.4039, Acc: 0.8542\n",
            "Epoch 138/150 — Loss: 0.4015, Acc: 0.8560\n",
            "Epoch 139/150 — Loss: 0.4063, Acc: 0.8531\n",
            "Epoch 140/150 — Loss: 0.4072, Acc: 0.8532\n",
            "Epoch 141/150 — Loss: 0.4026, Acc: 0.8541\n",
            "Epoch 142/150 — Loss: 0.4169, Acc: 0.8510\n",
            "Epoch 143/150 — Loss: 0.4095, Acc: 0.8530\n",
            "Epoch 144/150 — Loss: 0.4011, Acc: 0.8532\n",
            "Epoch 145/150 — Loss: 0.4201, Acc: 0.8498\n",
            "Epoch 146/150 — Loss: 0.4139, Acc: 0.8516\n",
            "Epoch 147/150 — Loss: 0.4066, Acc: 0.8530\n",
            "Epoch 148/150 — Loss: 0.4011, Acc: 0.8555\n",
            "Epoch 149/150 — Loss: 0.4076, Acc: 0.8524\n",
            "Epoch 150/150 — Loss: 0.4070, Acc: 0.8547\n"
          ]
        }
      ],
      "source": [
        "epochs = 150\n",
        "train_loss = []\n",
        "train_acc = []\n",
        "test_loss = []\n",
        "test_acc = []\n",
        "model.train()  # set training mode\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0\n",
        "    epoch_correct = 0\n",
        "    epoch_total = 0\n",
        "\n",
        "    for x, y in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(x)                      # call model safely\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # accumulate metrics\n",
        "        epoch_loss += loss.item() * x.size(0)\n",
        "        epoch_correct += (y_pred.argmax(1) == y).sum().item()\n",
        "        epoch_total += x.size(0)\n",
        "\n",
        "    train_loss.append(epoch_loss / epoch_total)\n",
        "    train_acc.append(epoch_correct / epoch_total)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} — Loss: {train_loss[-1]:.4f}, Acc: {train_acc[-1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()  # switch to evaluation mode\n",
        "test_loss = []\n",
        "test_acc = []\n",
        "epoch_test_loss = 0\n",
        "epoch_test_correct = 0\n",
        "epoch_test_total = 0\n",
        "\n",
        "with torch.no_grad():  # disable gradient tracking\n",
        "    for x, y in test_dataloader:\n",
        "        y_pred = model(x)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "\n",
        "        epoch_test_loss += loss.item() * x.size(0)\n",
        "        epoch_test_correct += (y_pred.argmax(1) == y).sum().item()\n",
        "        epoch_test_total += x.size(0)\n",
        "\n",
        "test_loss.append(epoch_test_loss / epoch_test_total)\n",
        "test_acc.append(epoch_test_correct / epoch_test_total)\n",
        "\n",
        "print(\n",
        "    f\"Epoch {epoch+1}/{epochs} \"\n",
        "    f\"— Train Acc: {train_acc[-1]:.4f} \"\n",
        "    f\"| Val Acc: {test_acc[-1]:.4f}\"\n",
        ")\n",
        "\n",
        "model.train()  # switch back for next epoch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFwyplB8P2aw",
        "outputId": "a7b9d6f0-bddc-40df-df51-5425af0b376b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 150/150 — Train Acc: 0.8547 | Val Acc: 0.8330\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv2): Conv2d(6, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv3): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (fc1): Linear(in_features=64, out_features=1024, bias=True)\n",
              "  (fc2): Linear(in_features=1024, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, 'model.pth')"
      ],
      "metadata": {
        "id": "M2DObbgamdTQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#what i learnt:\n",
        "1. calling model.foward(x) directly just pulls out the forward method and doesn't taking into account other stuff built into the model like train and evaluate.\n",
        "2. Don't assume the way you learnt how to build stuff is the standard way.\n",
        "\n",
        "# what i could do better:\n",
        "1. Look for the standard way a framework is used and build adaptations from there.\n",
        "2. Don't try to adapt with limited knowledge. Recipe for error."
      ],
      "metadata": {
        "id": "7CAIGp8_B_87"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0FKMQySDRYP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}