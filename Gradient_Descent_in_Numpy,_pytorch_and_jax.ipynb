{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Gradient Descent Implementation\n",
        "\n",
        "Libraries: NumPy | PyTorch | JAX  \n",
        "Purpose: Demonstrate linear regression using gradient descent with fully calculated gradients.  \n",
        "Scope: Single/multi-feature input, scalar output, MSE loss."
      ],
      "metadata": {
        "id": "c5o4Al7mNP4M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "kfk5JJST-nhh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import jax\n",
        "import jax.numpy as jnp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple gradient descent example in NumPy\n",
        "\n",
        "\n",
        "# Input features\n",
        "x = np.array([2, 2])\n",
        "\n",
        "# Parameters (weights and bias)\n",
        "w = np.array([0.1, 0.01])\n",
        "b = np.array([0.0])\n",
        "\n",
        "# Target output\n",
        "y = 0.1\n",
        "\n",
        "# Linear model: y_pred = w*x + b\n",
        "simple_node = lambda x, w, b: np.dot(x, w) + b\n",
        "\n",
        "# Learning rate\n",
        "lr = 0.1\n",
        "\n",
        "# Number of training iterations\n",
        "epochs = 100\n",
        "\n",
        "for r in range(epochs):\n",
        "    # Forward pass: compute predicted output\n",
        "    y_pred = simple_node(x, w, b)\n",
        "    print(f\"{y_pred} is the prediction for epoch {r}\")\n",
        "\n",
        "    # Compute mean squared error (MSE) loss\n",
        "    error = np.mean((y - y_pred) ** 2)\n",
        "\n",
        "    # Compute gradients using chain rule:\n",
        "    # Let u = (y - y_pred)\n",
        "    # dL/du = 2 * u\n",
        "    # du/dw = -x, du/db = -1\n",
        "    # Therefore:\n",
        "    # dL/dw = -2 * (y - y_pred) * x\n",
        "    # dL/db = -2 * (y - y_pred)\n",
        "    dw = -2 * (y - y_pred) * x\n",
        "    db = -2 * (y - y_pred)\n",
        "\n",
        "    # Update parameters using gradient descent\n",
        "    w = w - lr * dw\n",
        "    b = b - lr * db\n",
        "\n",
        "    # Print current loss\n",
        "    print(f\"error is {error}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlqTFEw2hXXm",
        "outputId": "c56de8f0-629c-4357-ca1f-5f5625c15ad1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.22] is the prediction for epoch 0\n",
            "error is 0.0144\n",
            "[0.004] is the prediction for epoch 1\n",
            "error is 0.009216\n",
            "[0.1768] is the prediction for epoch 2\n",
            "error is 0.005898240000000005\n",
            "[0.03856] is the prediction for epoch 3\n",
            "error is 0.0037748736000000043\n",
            "[0.149152] is the prediction for epoch 4\n",
            "error is 0.002415919104000003\n",
            "[0.0606784] is the prediction for epoch 5\n",
            "error is 0.0015461882265600032\n",
            "[0.13145728] is the prediction for epoch 6\n",
            "error is 0.000989560464998402\n",
            "[0.07483418] is the prediction for epoch 7\n",
            "error is 0.0006333186975989775\n",
            "[0.12013266] is the prediction for epoch 8\n",
            "error is 0.0004053239664633469\n",
            "[0.08389387] is the prediction for epoch 9\n",
            "error is 0.0002594073385365428\n",
            "[0.1128849] is the prediction for epoch 10\n",
            "error is 0.0001660206966633877\n",
            "[0.08969208] is the prediction for epoch 11\n",
            "error is 0.00010625324586456811\n",
            "[0.10824634] is the prediction for epoch 12\n",
            "error is 6.800207735332346e-05\n",
            "[0.09340293] is the prediction for epoch 13\n",
            "error is 4.352132950612701e-05\n",
            "[0.10527766] is the prediction for epoch 14\n",
            "error is 2.7853650883921375e-05\n",
            "[0.09577788] is the prediction for epoch 15\n",
            "error is 1.7826336565709586e-05\n",
            "[0.1033777] is the prediction for epoch 16\n",
            "error is 1.140885540205406e-05\n",
            "[0.09729784] is the prediction for epoch 17\n",
            "error is 7.301667457314599e-06\n",
            "[0.10216173] is the prediction for epoch 18\n",
            "error is 4.673067172681295e-06\n",
            "[0.09827062] is the prediction for epoch 19\n",
            "error is 2.990762990516067e-06\n",
            "[0.10138351] is the prediction for epoch 20\n",
            "error is 1.9140883139302907e-06\n",
            "[0.0988932] is the prediction for epoch 21\n",
            "error is 1.2250165209154107e-06\n",
            "[0.10088544] is the prediction for epoch 22\n",
            "error is 7.840105733858432e-07\n",
            "[0.09929165] is the prediction for epoch 23\n",
            "error is 5.017667669669239e-07\n",
            "[0.10056668] is the prediction for epoch 24\n",
            "error is 3.211307308588313e-07\n",
            "[0.09954665] is the prediction for epoch 25\n",
            "error is 2.0552366774964448e-07\n",
            "[0.10036268] is the prediction for epoch 26\n",
            "error is 1.3153514735976038e-07\n",
            "[0.09970986] is the prediction for epoch 27\n",
            "error is 8.418249431023215e-08\n",
            "[0.10023211] is the prediction for epoch 28\n",
            "error is 5.387679635854729e-08\n",
            "[0.09981431] is the prediction for epoch 29\n",
            "error is 3.4481149669470265e-08\n",
            "[0.10014855] is the prediction for epoch 30\n",
            "error is 2.2067935788461795e-08\n",
            "[0.09988116] is the prediction for epoch 31\n",
            "error is 1.4123478904614229e-08\n",
            "[0.10009507] is the prediction for epoch 32\n",
            "error is 9.039026498952578e-09\n",
            "[0.09992394] is the prediction for epoch 33\n",
            "error is 5.784976959330495e-09\n",
            "[0.10006085] is the prediction for epoch 34\n",
            "error is 3.7023852539715164e-09\n",
            "[0.09995132] is the prediction for epoch 35\n",
            "error is 2.3695265625423112e-09\n",
            "[0.10003894] is the prediction for epoch 36\n",
            "error is 1.5164970000279438e-09\n",
            "[0.09996885] is the prediction for epoch 37\n",
            "error is 9.705580800171923e-10\n",
            "[0.10002492] is the prediction for epoch 38\n",
            "error is 6.21157171211003e-10\n",
            "[0.09998006] is the prediction for epoch 39\n",
            "error is 3.975405895747099e-10\n",
            "[0.10001595] is the prediction for epoch 40\n",
            "error is 2.5442597732701745e-10\n",
            "[0.09998724] is the prediction for epoch 41\n",
            "error is 1.6283262548943285e-10\n",
            "[0.10001021] is the prediction for epoch 42\n",
            "error is 1.0421288031340701e-10\n",
            "[0.09999183] is the prediction for epoch 43\n",
            "error is 6.66962434005805e-11\n",
            "[0.10000653] is the prediction for epoch 44\n",
            "error is 4.268559577644405e-11\n",
            "[0.09999477] is the prediction for epoch 45\n",
            "error is 2.731878129704025e-11\n",
            "[0.10000418] is the prediction for epoch 46\n",
            "error is 1.7484020030198604e-11\n",
            "[0.09999665] is the prediction for epoch 47\n",
            "error is 1.1189772819327108e-11\n",
            "[0.10000268] is the prediction for epoch 48\n",
            "error is 7.161454604369349e-12\n",
            "[0.09999786] is the prediction for epoch 49\n",
            "error is 4.583330946820152e-12\n",
            "[0.10000171] is the prediction for epoch 50\n",
            "error is 2.933331805964897e-12\n",
            "[0.09999863] is the prediction for epoch 51\n",
            "error is 1.877332355817534e-12\n",
            "[0.1000011] is the prediction for epoch 52\n",
            "error is 1.201492707723222e-12\n",
            "[0.09999912] is the prediction for epoch 53\n",
            "error is 7.689553329525975e-13\n",
            "[0.1000007] is the prediction for epoch 54\n",
            "error is 4.921314130974508e-13\n",
            "[0.09999944] is the prediction for epoch 55\n",
            "error is 3.149641043885993e-13\n",
            "[0.10000045] is the prediction for epoch 56\n",
            "error is 2.0157702681368817e-13\n",
            "[0.09999964] is the prediction for epoch 57\n",
            "error is 1.290092971647481e-13\n",
            "[0.10000029] is the prediction for epoch 58\n",
            "error is 8.256595018862894e-14\n",
            "[0.09999977] is the prediction for epoch 59\n",
            "error is 5.2842208123274636e-14\n",
            "[0.10000018] is the prediction for epoch 60\n",
            "error is 3.381901320297916e-14\n",
            "[0.09999985] is the prediction for epoch 61\n",
            "error is 2.1644168452356692e-14\n",
            "[0.10000012] is the prediction for epoch 62\n",
            "error is 1.3852267810161626e-14\n",
            "[0.09999991] is the prediction for epoch 63\n",
            "error is 8.865451397458092e-15\n",
            "[0.10000008] is the prediction for epoch 64\n",
            "error is 5.6738888943731796e-15\n",
            "[0.09999994] is the prediction for epoch 65\n",
            "error is 3.631288893736879e-15\n",
            "[0.10000005] is the prediction for epoch 66\n",
            "error is 2.3240248901183403e-15\n",
            "[0.09999996] is the prediction for epoch 67\n",
            "error is 1.4873759290334762e-15\n",
            "[0.10000003] is the prediction for epoch 68\n",
            "error is 9.51920593896346e-16\n",
            "[0.09999998] is the prediction for epoch 69\n",
            "error is 6.09229180367693e-16\n",
            "[0.10000002] is the prediction for epoch 70\n",
            "error is 3.899066754353235e-16\n",
            "[0.09999998] is the prediction for epoch 71\n",
            "error is 2.4954027245398723e-16\n",
            "[0.10000001] is the prediction for epoch 72\n",
            "error is 1.5970577465116014e-16\n",
            "[0.09999999] is the prediction for epoch 73\n",
            "error is 1.0221169600122914e-16\n",
            "[0.10000001] is the prediction for epoch 74\n",
            "error is 6.541548548568398e-17\n",
            "[0.09999999] is the prediction for epoch 75\n",
            "error is 4.1865910854509207e-17\n",
            "[0.10000001] is the prediction for epoch 76\n",
            "error is 2.6794182831948727e-17\n",
            "[0.1] is the prediction for epoch 77\n",
            "error is 1.7148276851535155e-17\n",
            "[0.1] is the prediction for epoch 78\n",
            "error is 1.097489701947298e-17\n",
            "[0.1] is the prediction for epoch 79\n",
            "error is 7.023933960055095e-18\n",
            "[0.1] is the prediction for epoch 80\n",
            "error is 4.495317663817868e-18\n",
            "[0.1] is the prediction for epoch 81\n",
            "error is 2.8770032483495217e-18\n",
            "[0.1] is the prediction for epoch 82\n",
            "error is 1.8412820337485634e-18\n",
            "[0.1] is the prediction for epoch 83\n",
            "error is 1.1784204835210285e-18\n",
            "[0.1] is the prediction for epoch 84\n",
            "error is 7.541890949910168e-19\n",
            "[0.1] is the prediction for epoch 85\n",
            "error is 4.826809860843919e-19\n",
            "[0.1] is the prediction for epoch 86\n",
            "error is 3.089158033261249e-19\n",
            "[0.1] is the prediction for epoch 87\n",
            "error is 1.9770609191441232e-19\n",
            "[0.1] is the prediction for epoch 88\n",
            "error is 1.2653189685061883e-19\n",
            "[0.1] is the prediction for epoch 89\n",
            "error is 8.098041240471202e-20\n",
            "[0.1] is the prediction for epoch 90\n",
            "error is 5.1827452565291337e-20\n",
            "[0.1] is the prediction for epoch 91\n",
            "error is 3.3169569641786455e-20\n",
            "[0.1] is the prediction for epoch 92\n",
            "error is 2.122852457074333e-20\n",
            "[0.1] is the prediction for epoch 93\n",
            "error is 1.3586255725275732e-20\n",
            "[0.1] is the prediction for epoch 94\n",
            "error is 8.695203664176469e-21\n",
            "[0.1] is the prediction for epoch 95\n",
            "error is 5.5649291027590784e-21\n",
            "[0.1] is the prediction for epoch 96\n",
            "error is 3.56155462576581e-21\n",
            "[0.1] is the prediction for epoch 97\n",
            "error is 2.2793957555710148e-21\n",
            "[0.1] is the prediction for epoch 98\n",
            "error is 1.4588147677169628e-21\n",
            "[0.1] is the prediction for epoch 99\n",
            "error is 9.336421298084152e-22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AawStHSY_2R6",
        "outputId": "13a71b76-4768-4e1f-dc6f-e49e2d8375fc"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.1])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch"
      ],
      "metadata": {
        "id": "6B-15ruOODkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Descent in Pytorch\n",
        "# Input features\n",
        "x = torch.tensor([2, 2], dtype=torch.float64)\n",
        "\n",
        "# Parameters (weights and bias), set requires_grad=True for autograd\n",
        "w = torch.tensor([0.1, 0.01], dtype=torch.float64, requires_grad=True)\n",
        "b = torch.tensor([0.0], dtype=torch.float64, requires_grad=True)\n",
        "\n",
        "# Target output\n",
        "y = torch.tensor([0.1])\n",
        "\n",
        "# Learning rate\n",
        "lr = 0.1\n",
        "\n",
        "# Number of features (for averaging loss if needed)\n",
        "m = x.shape[0]\n",
        "\n",
        "# Training loop\n",
        "for i in range(100):\n",
        "    # Define linear model: y_pred = w*x + b\n",
        "    node = lambda w, b: torch.dot(x, w) + b\n",
        "    y_pred = node(w, b)\n",
        "\n",
        "    # Compute mean squared error (MSE) loss\n",
        "    loss = ((y - y_pred) ** 2) / m\n",
        "\n",
        "    # Backward pass: compute gradients automatically\n",
        "    loss.backward()\n",
        "\n",
        "    # Retrieve gradients\n",
        "    dw = w.grad\n",
        "    db = b.grad\n",
        "\n",
        "    # Gradient descent parameter update\n",
        "    w.data -= lr * dw\n",
        "    b.data -= lr * db\n",
        "\n",
        "    # Clear gradients for next iteration\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "\n",
        "    # Print current loss\n",
        "    print(f\"Epoch {i}: loss = {loss.item()}\")\n",
        "\n",
        "# Final prediction\n",
        "print(f\"Final prediction: {y_pred.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d-daHXo39CX",
        "outputId": "9191679a-6a6f-451a-d6f8-da33b95da932"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: loss = 0.0071999998211860665\n",
            "Epoch 1: loss = 7.199999821186084e-05\n",
            "Epoch 2: loss = 7.19999982118595e-07\n",
            "Epoch 3: loss = 7.199999821185784e-09\n",
            "Epoch 4: loss = 7.19999982119078e-11\n",
            "Epoch 5: loss = 7.199999821224086e-13\n",
            "Epoch 6: loss = 7.19999982089102e-15\n",
            "Epoch 7: loss = 7.199999842540368e-17\n",
            "Epoch 8: loss = 7.199999859193714e-19\n",
            "Epoch 9: loss = 7.199997860792427e-21\n",
            "Epoch 10: loss = 7.200014514144952e-23\n",
            "Epoch 11: loss = 7.200147741658488e-25\n",
            "Epoch 12: loss = 7.200147741658488e-27\n",
            "Epoch 13: loss = 7.18849499882647e-29\n",
            "Epoch 14: loss = 7.288681874533494e-31\n",
            "Epoch 15: loss = 7.800016274768305e-33\n",
            "Epoch 16: loss = 0.0\n",
            "Epoch 17: loss = 0.0\n",
            "Epoch 18: loss = 0.0\n",
            "Epoch 19: loss = 0.0\n",
            "Epoch 20: loss = 0.0\n",
            "Epoch 21: loss = 0.0\n",
            "Epoch 22: loss = 0.0\n",
            "Epoch 23: loss = 0.0\n",
            "Epoch 24: loss = 0.0\n",
            "Epoch 25: loss = 0.0\n",
            "Epoch 26: loss = 0.0\n",
            "Epoch 27: loss = 0.0\n",
            "Epoch 28: loss = 0.0\n",
            "Epoch 29: loss = 0.0\n",
            "Epoch 30: loss = 0.0\n",
            "Epoch 31: loss = 0.0\n",
            "Epoch 32: loss = 0.0\n",
            "Epoch 33: loss = 0.0\n",
            "Epoch 34: loss = 0.0\n",
            "Epoch 35: loss = 0.0\n",
            "Epoch 36: loss = 0.0\n",
            "Epoch 37: loss = 0.0\n",
            "Epoch 38: loss = 0.0\n",
            "Epoch 39: loss = 0.0\n",
            "Epoch 40: loss = 0.0\n",
            "Epoch 41: loss = 0.0\n",
            "Epoch 42: loss = 0.0\n",
            "Epoch 43: loss = 0.0\n",
            "Epoch 44: loss = 0.0\n",
            "Epoch 45: loss = 0.0\n",
            "Epoch 46: loss = 0.0\n",
            "Epoch 47: loss = 0.0\n",
            "Epoch 48: loss = 0.0\n",
            "Epoch 49: loss = 0.0\n",
            "Epoch 50: loss = 0.0\n",
            "Epoch 51: loss = 0.0\n",
            "Epoch 52: loss = 0.0\n",
            "Epoch 53: loss = 0.0\n",
            "Epoch 54: loss = 0.0\n",
            "Epoch 55: loss = 0.0\n",
            "Epoch 56: loss = 0.0\n",
            "Epoch 57: loss = 0.0\n",
            "Epoch 58: loss = 0.0\n",
            "Epoch 59: loss = 0.0\n",
            "Epoch 60: loss = 0.0\n",
            "Epoch 61: loss = 0.0\n",
            "Epoch 62: loss = 0.0\n",
            "Epoch 63: loss = 0.0\n",
            "Epoch 64: loss = 0.0\n",
            "Epoch 65: loss = 0.0\n",
            "Epoch 66: loss = 0.0\n",
            "Epoch 67: loss = 0.0\n",
            "Epoch 68: loss = 0.0\n",
            "Epoch 69: loss = 0.0\n",
            "Epoch 70: loss = 0.0\n",
            "Epoch 71: loss = 0.0\n",
            "Epoch 72: loss = 0.0\n",
            "Epoch 73: loss = 0.0\n",
            "Epoch 74: loss = 0.0\n",
            "Epoch 75: loss = 0.0\n",
            "Epoch 76: loss = 0.0\n",
            "Epoch 77: loss = 0.0\n",
            "Epoch 78: loss = 0.0\n",
            "Epoch 79: loss = 0.0\n",
            "Epoch 80: loss = 0.0\n",
            "Epoch 81: loss = 0.0\n",
            "Epoch 82: loss = 0.0\n",
            "Epoch 83: loss = 0.0\n",
            "Epoch 84: loss = 0.0\n",
            "Epoch 85: loss = 0.0\n",
            "Epoch 86: loss = 0.0\n",
            "Epoch 87: loss = 0.0\n",
            "Epoch 88: loss = 0.0\n",
            "Epoch 89: loss = 0.0\n",
            "Epoch 90: loss = 0.0\n",
            "Epoch 91: loss = 0.0\n",
            "Epoch 92: loss = 0.0\n",
            "Epoch 93: loss = 0.0\n",
            "Epoch 94: loss = 0.0\n",
            "Epoch 95: loss = 0.0\n",
            "Epoch 96: loss = 0.0\n",
            "Epoch 97: loss = 0.0\n",
            "Epoch 98: loss = 0.0\n",
            "Epoch 99: loss = 0.0\n",
            "Final prediction: 0.10000000149011612\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "JAX: Simple Gradient Descent"
      ],
      "metadata": {
        "id": "NZNaYpyhJMUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Gradient Descent in Jax\n",
        "\n",
        "# Input features\n",
        "x = jnp.array([2.0, 2.0])\n",
        "\n",
        "# Parameters (weights and bias)\n",
        "w = jnp.array([0.1, 0.01])\n",
        "b = jnp.array([0.0])\n",
        "\n",
        "# Target output\n",
        "y = 0.1\n",
        "\n",
        "# Learning rate\n",
        "lr = 0.1\n",
        "\n",
        "# Number of features (for averaging loss)\n",
        "m = x.shape[0]\n",
        "\n",
        "# Linear model\n",
        "node = lambda w, b: jnp.dot(x, w) + b\n",
        "\n",
        "# Loss function (mean squared error)\n",
        "error = lambda w, b: jnp.mean((y - node(w, b)) ** 2)\n",
        "\n",
        "# Training loop\n",
        "epochs = 100\n",
        "for r in range(epochs):\n",
        "    # Compute gradients w.r.t weights and bias NB: grad was used here unlike vmap seen in previous codes. b is a scalar, w vector.\n",
        "    dw, db = jax.grad(error, argnums=(0, 1))(w, b)\n",
        "\n",
        "    # Gradient descent update\n",
        "    w = w - lr * dw\n",
        "    b = b - lr * db\n",
        "\n",
        "    # Compute loss and prediction after update\n",
        "    loss = error(w, b)\n",
        "    y_pred = node(w, b)\n",
        "\n",
        "    # Print current loss and prediction\n",
        "    print(f\"Epoch {r}: loss = {loss}, y_pred = {y_pred}\")\n",
        "# to understand the loss function better in jax, see Partial derivative Chain rule in Computational Multivariate Calculus Notebook."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCEYU6Z1OovM",
        "outputId": "1da8528d-f5fe-47e7-8d8c-f969a4a4d777"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss is 0.009216000325977802 and y_pred is [0.004]\n",
            "loss is 0.005898241885006428 and y_pred is [0.17680001]\n",
            "loss is 0.003774875309318304 and y_pred is [0.03855999]\n",
            "loss is 0.002415921539068222 and y_pred is [0.14915203]\n",
            "loss is 0.001546190120279789 and y_pred is [0.06067838]\n",
            "loss is 0.000989562482573092 and y_pred is [0.13145731]\n",
            "loss is 0.0006333203054964542 and y_pred is [0.07483415]\n",
            "loss is 0.0004053249431308359 and y_pred is [0.12013268]\n",
            "loss is 0.0002594076213426888 and y_pred is [0.08389387]\n",
            "loss is 0.00016602102550677955 and y_pred is [0.11288492]\n",
            "loss is 0.00010625358117977157 and y_pred is [0.08969206]\n",
            "loss is 6.800224218750373e-05 and y_pred is [0.10824635]\n",
            "loss is 4.352135874796659e-05 and y_pred is [0.09340293]\n",
            "loss is 2.7853731808136217e-05 and y_pred is [0.10527766]\n",
            "loss is 1.7826338080340065e-05 and y_pred is [0.09577788]\n",
            "loss is 1.1408896170905791e-05 and y_pred is [0.10337771]\n",
            "loss is 7.301718142116442e-06 and y_pred is [0.09729783]\n",
            "loss is 4.673157491197344e-06 and y_pred is [0.10216175]\n",
            "loss is 2.990820803461247e-06 and y_pred is [0.0982706]\n",
            "loss is 1.914088215926313e-06 and y_pred is [0.10138351]\n",
            "loss is 1.2249867040736717e-06 and y_pred is [0.09889321]\n",
            "loss is 7.839677778065379e-07 and y_pred is [0.10088542]\n",
            "loss is 5.017372473048454e-07 and y_pred is [0.09929167]\n",
            "loss is 3.2109664971358143e-07 and y_pred is [0.10056666]\n",
            "loss is 2.0549509827105794e-07 and y_pred is [0.09954669]\n",
            "loss is 1.3151471023320482e-07 and y_pred is [0.10036265]\n",
            "loss is 8.417286778694688e-08 and y_pred is [0.09970988]\n",
            "loss is 5.387063595208019e-08 and y_pred is [0.1002321]\n",
            "loss is 3.447831531389056e-08 and y_pred is [0.09981432]\n",
            "loss is 2.206700600027034e-08 and y_pred is [0.10014855]\n",
            "loss is 1.4120405289475002e-08 and y_pred is [0.09988117]\n",
            "loss is 9.036775772131023e-09 and y_pred is [0.10009506]\n",
            "loss is 5.783309831031147e-09 and y_pred is [0.09992395]\n",
            "loss is 3.7007743536321414e-09 and y_pred is [0.10006084]\n",
            "loss is 2.3684956218517073e-09 and y_pred is [0.09995133]\n",
            "loss is 1.515489067571707e-09 and y_pred is [0.10003893]\n",
            "loss is 9.69448965548736e-10 and y_pred is [0.09996887]\n",
            "loss is 6.207443448147387e-10 and y_pred is [0.10002492]\n",
            "loss is 3.9781117289194867e-10 and y_pred is [0.09998006]\n",
            "loss is 2.549321309608388e-10 and y_pred is [0.10001597]\n",
            "loss is 1.6308043804258432e-10 and y_pred is [0.09998723]\n",
            "loss is 1.0434103581147269e-10 and y_pred is [0.10001022]\n",
            "loss is 6.655925810505892e-11 and y_pred is [0.09999184]\n",
            "loss is 4.2597925187237706e-11 and y_pred is [0.10000653]\n",
            "loss is 2.7278235226191327e-11 and y_pred is [0.09999478]\n",
            "loss is 1.7470525026652695e-11 and y_pred is [0.10000418]\n",
            "loss is 1.1191103599372809e-11 and y_pred is [0.09999666]\n",
            "loss is 7.194245199571014e-12 and y_pred is [0.10000268]\n",
            "loss is 4.604316927725449e-12 and y_pred is [0.09999786]\n",
            "loss is 2.936539900133539e-12 and y_pred is [0.10000172]\n",
            "loss is 1.879385536085465e-12 and y_pred is [0.09999863]\n",
            "loss is 1.2159162565694714e-12 and y_pred is [0.1000011]\n",
            "loss is 7.860934125858421e-13 and y_pred is [0.09999911]\n",
            "loss is 5.223044219349049e-13 and y_pred is [0.10000072]\n",
            "loss is 3.464450948342801e-13 and y_pred is [0.09999941]\n",
            "loss is 2.2737367544323206e-13 and y_pred is [0.10000048]\n",
            "loss is 1.5010215292932116e-13 and y_pred is [0.09999961]\n",
            "loss is 1.0264011862659572e-13 and y_pred is [0.10000032]\n",
            "loss is 6.800116025829084e-14 and y_pred is [0.09999974]\n",
            "loss is 4.3520742565306136e-14 and y_pred is [0.10000021]\n",
            "loss is 2.936539900133539e-14 and y_pred is [0.09999983]\n",
            "loss is 2.0039525594484076e-14 and y_pred is [0.10000014]\n",
            "loss is 1.4210854715202004e-14 and y_pred is [0.09999988]\n",
            "loss is 9.381384558082573e-15 and y_pred is [0.1000001]\n",
            "loss is 6.716849298982197e-15 and y_pred is [0.09999992]\n",
            "loss is 4.496403249731884e-15 and y_pred is [0.10000007]\n",
            "loss is 4.496403249731884e-15 and y_pred is [0.09999993]\n",
            "loss is 4.496403249731884e-15 and y_pred is [0.10000007]\n",
            "loss is 4.496403249731884e-15 and y_pred is [0.09999993]\n",
            "loss is 4.496403249731884e-15 and y_pred is [0.10000007]\n",
            "loss is 4.496403249731884e-15 and y_pred is [0.09999993]\n",
            "loss is 4.496403249731884e-15 and y_pred is [0.10000007]\n",
            "loss is 4.496403249731884e-15 and y_pred is [0.09999993]\n",
            "loss is 4.496403249731884e-15 and y_pred is [0.10000007]\n",
            "loss is 4.496403249731884e-15 and y_pred is [0.09999993]\n",
            "loss is 4.496403249731884e-15 and y_pred is [0.10000007]\n",
            "loss is 4.496403249731884e-15 and y_pred is [0.09999993]\n",
            "loss is 4.496403249731884e-15 and y_pred is [0.10000007]\n",
            "loss is 4.496403249731884e-15 and y_pred is [0.09999993]\n",
            "loss is 4.496403249731884e-15 and y_pred is [0.10000007]\n",
            "loss is 4.496403249731884e-15 and y_pred is [0.09999993]\n",
            "loss is 4.496403249731884e-15 and y_pred is [0.10000007]\n",
            "loss is 4.496403249731884e-15 and y_pred is [0.09999993]\n",
            "loss is 4.496403249731884e-15 and y_pred is [0.10000007]\n",
            "loss is 4.496403249731884e-15 and y_pred is [0.09999993]\n",
            "loss is 4.496403249731884e-15 and y_pred is [0.10000007]\n",
            "loss is 4.496403249731884e-15 and y_pred is [0.09999993]\n",
            "loss is 4.496403249731884e-15 and y_pred is [0.10000007]\n",
            "loss is 4.496403249731884e-15 and y_pred is [0.09999993]\n",
            "loss is 4.496403249731884e-15 and y_pred is [0.10000007]\n",
            "loss is 4.496403249731884e-15 and y_pred is [0.09999993]\n",
            "loss is 4.496403249731884e-15 and y_pred is [0.10000007]\n",
            "loss is 4.496403249731884e-15 and y_pred is [0.09999993]\n",
            "loss is 4.496403249731884e-15 and y_pred is [0.10000007]\n",
            "loss is 4.496403249731884e-15 and y_pred is [0.09999993]\n",
            "loss is 4.496403249731884e-15 and y_pred is [0.10000007]\n",
            "loss is 4.496403249731884e-15 and y_pred is [0.09999993]\n",
            "loss is 4.496403249731884e-15 and y_pred is [0.10000007]\n",
            "loss is 4.496403249731884e-15 and y_pred is [0.09999993]\n",
            "loss is 4.496403249731884e-15 and y_pred is [0.10000007]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_JEg7otQSYt4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}